{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五章 决策树\n",
    "\n",
    "## 5.1 决策树模型与学习\n",
    "\n",
    "### 5.1.1 决策树模型\n",
    "定义 5.1 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性，叶结点表示一个类。\n",
    "\n",
    "### 5.1.4 决策树学习\n",
    "假设给定训练数据集\n",
    "$$D={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$$\n",
    "其中，$x_i = (x_i^{(1)},x_i^{(2)},…,x_i^{(N)}^T$为输入实例（特征向量），n为特征个数，$y_i \\in \\{1,2,…,K\\}$为类标记，$i=1,2,…,N$，N为样本容量。决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。\n",
    "\n",
    "决策树学习用损失函数表示这一目标，通常是正则化的极大似然函数，决策树学习的策略是以损失函数为目标函数的最小化。\n",
    "\n",
    "决策树学习算法包含特征选择，决策树的生成与决策树的剪枝过程。决策树的生成对应于模型的局部选择，决策树的剪枝对应模型的全局选择。\n",
    "\n",
    "决策树学习常用的算法有ID3，C4.5与CART。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 特征选择\n",
    "### 5.2.1 特征选择问题\n",
    "特征选择在于选取队训练数据具有分类能力的特征，从而提高决策树学习的学习效率。决定是否扔掉无效特征的准则是信息增益或信息增益比。\n",
    "\n",
    "### 5.2.2 信息增益（information gain）\n",
    "在信息论与概率统计中，熵(entropy)是表示随机变量不确定性的变量。设X是一个取有限值的离散随机变量，其概率分布为：\n",
    "$$P(X=x_i)=p_i,\\quad i=1,2,…,n$$\n",
    "则随机变量X的熵定义为：\n",
    "$$H(X)=-\\sum_{i=1}^n p_ilog p_i$$\n",
    "若$p_i=0$，定义$0log0=0$，以2为底与以e为底的熵的单位分别称作比特（bit）和纳特（nat）。熵仅依赖于X的分布，与X的取值无关，因此也可即为$H(p)$，熵越大，随机变量的不确定性就越大。\n",
    "条件熵$H(Y|X)$:\n",
    "$$H(Y|X)=\\sum_{i=1}^n p_iH(Y|X=x_i),\\quad p_i=P(X=x_i)$$\n",
    "当熵与条件熵中的概率由数据估计（极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵（empirical entropy）与经验条件熵（empirical conditional entropy）\n",
    "\n",
    "信息增益表示得知特征X的信息而使得Y的信息不确定性减少的程度。\n",
    "\n",
    "定义 5.2（信息增益） 特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即：\n",
    "$$g(D,A)=H(D)-H(D|A)$$\n",
    "\n",
    "### 5.2.3 信息增益比（information gain ratio）\n",
    "定义5.3 （信息增益比）特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益g(D,A)与训练数据集D关于特征A的值的熵$H_A(D)$之比，即：\n",
    "$$g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 决策树的生成\n",
    "算法5.2 （ID3）：\n",
    "\n",
    "输入：训练数据集D，特征集A阈值$\\epsilon$\n",
    "\n",
    "输出：决策树T\n",
    "\n",
    "（1）若D中所有实例属于同一类$C_k$，则T为单结点树，并将类$C_k$作为该节点的类标记，返回T\n",
    "\n",
    "（2）若$A = \\varPhi$，则T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T\n",
    "\n",
    "（3）否则，计算A中个特征对D的信息增益，选择信息增益最大的特征$A_g$\n",
    "\n",
    "（4）如果$A_g$的信息增益小于阈值$\\epsilon$，则置T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T\n",
    "\n",
    "（5）否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将D分割成若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T\n",
    "\n",
    "（6）递归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法5.3（C4.5的生成算法）\n",
    "输入：训练数据集D，特征集A阈值$\\epsilon$\n",
    "\n",
    "输出：决策树T\n",
    "\n",
    "（1）若D中所有实例属于同一类$C_k$，则T为单结点树，并将类$C_k$作为该节点的类标记，返回T\n",
    "\n",
    "（2）若$A = \\varPhi$，则T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T\n",
    "\n",
    "（3）否则，计算A中个特征对D的信息增益比，选择信息增益比最大的特征$A_g$\n",
    "\n",
    "（4）如果$A_g$的信息增益比小于阈值$\\epsilon$，则置T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T\n",
    "\n",
    "（5）否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将D分割成若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T\n",
    "\n",
    "（6）递归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树T的叶结点个数|T|，t是树T的叶结点，该叶结点有$N_t$个样本点，其中k类的样本点有$N_tk$个，$k=1,2,…,K$，$H_t(T)$为叶结点t上的经验熵，$\\alpha \\geqslant 0$为参数，损失函数定义为：\n",
    "$$C_\\alpha (T)=\\sum_{t=1}^{|T|}N_tH_t(T)+\\alpha |T|=C(T)=\\alpha |T|$$\n",
    "其中，经验熵为：\n",
    "$$H_t(T)=-\\sum_k \\frac{N_{tk}}{N_t}log \\frac{N_{tk}}{N_t}$$\n",
    "算法5.4（树的剪枝算法）\n",
    "\n",
    "输入：生成算法产生的整个树T，参数$\\alpha$\n",
    "\n",
    "输出：修剪后的子树$T_\\alpha$\n",
    "\n",
    "（1）计算每个结点的经验熵\n",
    "\n",
    "（2）递归地从树的叶结点向上回缩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "习题 5.1 C4.5生成决策树\n",
    "\n",
    "经验熵$H(D)=-\\frac{6}{15}log\\frac{6}{15}-\\frac{9}{15}log\\frac{9}{15}=0.971$\n",
    "\n",
    "用$A_1,A_2,A_3,A_4$分别表示年龄，有工作，有自己的房子和信贷情况四个特征\n",
    "\n",
    "$g(D,A_1) = H(D)-[\\frac{5}{15}H(D_1)+\\frac{5}{15}H(D_2)+\\frac{5}{15}H(D_3)]=0.971-[\\frac{5}{15}(-\\frac{2}{5}log_2\\frac{2}{5}-\\frac{3}{5}log_2\\frac{3}{5})+\\frac{5}{15}(-\\frac{2}{5}log_2\\frac{2}{5}-\\frac{3}{5}log_2\\frac{3}{5})+\\frac{5}{15}(-\\frac{1}{5}log_2\\frac{1}{5}-\\frac{4}{5}log_2\\frac{4}{5})=0.971-0.888=0.083$\n",
    "\n",
    "$H_{A_1}(D)=-\\frac{5}{15}log_2\\frac{5}{15}-\\frac{5}{15}log_2\\frac{5}{15}-\\frac{5}{15}log_2\\frac{5}{15}=1.585$\n",
    "\n",
    "$g_R(D,A_1) = 0.083/1.585=0.0524$\n",
    "\n",
    "$g(D,A_2)=0.324$\n",
    "\n",
    "$H_{A_2}(D)=-\\frac{5}{15}log_2\\frac{5}{15}-\\frac{10}{15}log_2\\frac{10}{15}=0.9183$\n",
    "\n",
    "$g_R(D,A_2) = 0.324/0.9183=0.3528$\n",
    "\n",
    "$g(D,A_3)=0.420$\n",
    "\n",
    "$H_{A_3}(D)=-\\frac{6}{15}log_2\\frac{6}{15}-\\frac{9}{15}log_2\\frac{9}{15}=0.971$\n",
    "\n",
    "$g_R(D,A_3) = 0.420/0.971=0.4325$\n",
    "\n",
    "$g(D,A_4)=0.363$\n",
    "\n",
    "$H_{A_4}(D)=-\\frac{5}{15}log_2\\frac{5}{15}-\\frac{6}{15}log_2\\frac{6}{15}-\\frac{4}{15}log_2\\frac{4}{15}=1.5656$\n",
    "\n",
    "$g_R(D,A_4) = 0.420/0.971=0.2319$\n",
    "\n",
    "由上式可看出$A_3$的信息增益比最大，所以选择特征$A_3$为最优特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
