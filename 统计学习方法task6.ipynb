{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6章 逻辑回归与最大熵模型\n",
    "\n",
    "## 6.1 逻辑回归\n",
    "\n",
    "### 6.1.1 logistic 分布\n",
    "**定义 6.1（logistic distribution）** 设X是连续随机变量，X服从logistic分布是指X具有下列分布函数和密度函数：\n",
    "$$F(x)=P(X \\leqslant x)=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}$$\n",
    "$$f(x)=F'(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma(1+e^{-(x-\\mu)/\\gamma})^2}$$\n",
    "$\\mu$为位置参数，$\\gamma$为形状参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 二项逻辑回归模型\n",
    "二项逻辑回归模型（binomial logistic regression model）是一种分类模型，由条件概率分布P(Y|X)表示，形式为参数化的logistic分布。\n",
    "\n",
    "**定义6.2（logistic model）** 二项逻辑回归模型是如下的条件概率分布：\n",
    "$$P(Y=1|x) = \\frac{e^{(w\\cdot x+b)}}{1+e^{(w\\cdot x+b)}}$$\n",
    "$$P(Y=0|x) = \\frac{1}{1+e^{(w\\cdot x+b)}}$$\n",
    "一个事件的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值，即对一个发生概率为p的事件，该事件的几率是$\\frac{p}{1-p}$，该事件的对数几率（log odds）或logit函数是\n",
    "$$logit(p) = log \\frac{p}{1-p}$$\n",
    "对logistic回归而言，\n",
    "$$log \\frac{P(Y=1|x}{1-P(Y=1|x}=w\\cdot x+b$$\n",
    "对于逻辑回归模型而言，输出Y=1的对数几率是x的线性函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 模型参数估计\n",
    "对于逻辑回归模型学习时，可以应用极大似然估计法估计模型参数，从而得到逻辑回归模型\n",
    "设:\n",
    "$$P(Y=1|x)=\\pi(x),\\quad P(Y=0|x)=1-\\pi(x)$$\n",
    "似然函数为：\n",
    "$$\\prod_{i=1}^N[\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}$$\n",
    "对数似然函数为：\n",
    "$$L(w) = \\sum_{i=1}^N[y_ilog\\pi(x_i)+(1-y_i)log(1-\\pi(x_i))]$$\n",
    "$$=\\sum_{i=1}^N[y_ilog\\frac{\\pi(x_i)}{1-\\pi(x_i)}+log(1-\\pi(x_i)]$$\n",
    "$$=\\sum_{i=1}^N[y_i(w\\cdot x)-log(1+e^{(w\\cdot x)})$$\n",
    "对上式求极大值得到w的估计值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 多项逻辑回归\n",
    "$$P(Y=k|x)=\\frac{e^{(w_k\\cdot x)}}{1+\\sum_{k=1}^{K-1}e^{w_k\\cdot x}},\\quad k=1,2,…,K-1$$\n",
    "$$P(Y=K|x)=\\frac{1}{1+\\sum_{k=1}^{K-1}e^{w_k\\cdot x}},\\quad k=K$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 最大熵模型\n",
    "### 6.2.1 最大熵原理\n",
    "最大熵原理是概率魔性学习的一个准则。最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型时最好的模型。\n",
    "\n",
    "假设离散随机变量X的概率分布为P(X)，其熵为：\n",
    "$$H(P) = -\\sum_xP(x)log P(x)$$\n",
    "熵满足不等式：\n",
    "$$0\\leqslant H(P)\\leqslant log|X|$$\n",
    "即X服从均匀分布时，熵最大。\n",
    "### 6.2.2 最大熵模型的定义\n",
    "假设分类模型是一个条件概率分布P(Y|X),$X\\in \\mathcal{X}\\subseteq \\mathbb{R}^n$表示输入，$Y\\in\\mathcal{Y}$表示输出，$\\mathcal{X}$和$\\mathcal{Y}$分别是输入和输出的集合，对于给定的训练数据集T，学习的目标是用最大熵原理选择最好的分类模型。\n",
    "联合分布P(X,Y)的经验分布和边缘分布P(X)的经验分布如下，以$\\tilde{P}(X,Y)$和$\\tilde{P}(X)$表示。\n",
    "$$\\tilde{P}(X=x,Y=y)=\\frac{\\nu(X=x,Y=y)}{N}$$\n",
    "$$\\tilde{P}(X=x)=\\frac{\\nu(X=x)}{N}$$\n",
    "$\\nu$表示出现的频数\n",
    "用一个二值特征函数f(x,y)描述输入x和输出y之间的某个事实。\n",
    "$$f(x,y)=\\left\\{\\begin{matrix} 1,\\quad x与y满足某个事实\n",
    "\\\\0,\\quad否则\n",
    "\\end{matrix}\\right.$$\n",
    "特征函数f(x,y)关于经验分布$\\tilde{P}(X,Y)$的期望值，用$E_{\\tilde{P}}(f)$表示：\n",
    "$$E_\\tilde{P}(f)=\\sum_{x,y}\\tilde{P}(x,y)f(x,y)$$\n",
    "特征函数f(x,y)关于模型P(Y|X)与经验分布$\\tilde{P}(X)$的期望值，用$E_{P}(f)$表示：\n",
    "$$E_P(f)=\\sum_{x,y}\\tilde{P}(x)P(y|x)f(x,y)$$\n",
    "对于训练数据中的信息，应使上面两式相等，即$E_{\\tilde{P}}(f)=E_P(f)$，作为最大熵模型的约束条件。\n",
    "\n",
    "**定义6.3（最大熵模型）** 假设满足所有约束条件的模型集合为\n",
    "$$\\mathcal{C}\\equiv {P\\in \\mathcal{P}|E_P(f_i),\\quad i=1,2,…,n}$$\n",
    "定义在条件概率分布P(Y|X)上的熵为\n",
    "$$H(P)=-\\sum_{x,y}\\tilde{P}(x)P(y|x)log P(y|x)$$\n",
    "则模型集合$\\mathcal(C)$中条件上H(P)最大的模型成为最大熵模型，对数为自然对数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 最大熵模型的学习\n",
    "对于给定的数据集T以及特征函数$f_i(x,y),i=1,2,…,n$，最大熵模型的学习等价于约束最优化问题：\n",
    "$$\\mathop{\\max}_{P\\in \\mathcal(C)}H(P) = -\\sum_{x,y}\\tilde{P}(x)P(y|x)logP(y|x)$$\n",
    "$$s.t.E_P(f_i)=E_{\\tilde{P}}(f_i),\\quad i=1,2,…,n$$\n",
    "$$\\sum_y P(y|x) =1 $$\n",
    "利用拉格朗日乘子法将上述问题转化为对偶问题，引进拉格朗日乘子$w_i$，定义拉格朗日函数L(P,w):\n",
    "$$L(P,w)\\equiv -H(P)+w_o(1-\\sum_y P(y|x))+\\sum_{i=1}^n w_i(E_{\\tilde{P}}(f_i)-E_P(f_i))$$\n",
    "$$=\\sum_{x,y}\\tilde{P}(x)P(y|x)logP(y|x)+w_0(1-\\sum_yP(y|x))+\\sum_{i=1}^n w_i(\\sum_{x,y}\\tilde{P}(x,y)f_i(x,y)-\\sum_{x,y}\\tilde{P}(x)P(y|x)f_i(x,y))$$\n",
    "通过求解对偶问题$\\mathop{\\min}_{P\\in \\mathcal{C}}L(P,w)$的极小化来求解原问题,由于$\\mathop{\\min}_{P\\in \\mathcal{C}}L(P,w)$是w的函数，记作：\n",
    "$$\\mathcal{\\Phi} \\mathop{\\min}_{P\\in \\mathcal{C}}L(P,w)=L(P_w,w)$$\n",
    "其解记为：\n",
    "$$P_w = arg\\mathop{\\min}_{P\\in \\mathcal{C}}L(P,w)=P_w(y|x)$$\n",
    "求$L(P,w)$对$P(y|x)$的偏导数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L(P,w)}{\\partial P(y|x)}=\\sum_{x,y}\\tilde{P}(x)(logP(y|x)+1)-\\sum_y w_o-\\sum_{x,y}(\\tilde{P}(x)\\sum_{i=1}^n w_if_i(x,y ))$$\n",
    "令上述偏导数为0，得到：\n",
    "$$P(y|x) = e^{\\sum_{i=1}^n w_if_i(x,y)+w_0-1}=\\frac{exp(\\sum_{i=1}^n w_if_i(x,y)}{exp(1-w_0)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
